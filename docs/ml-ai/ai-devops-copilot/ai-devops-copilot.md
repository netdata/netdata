# MCP Clients

Model Context Protocol (MCP) clients like **Claude Desktop**, **Cursor**, **Visual Studio Code**, **JetBrains IDEs**, **Netdata Web Client**, **Claude Code**, and **Gemini CLI** can connect to Netdata’s MCP server to bring real observability data into your AI workflows. This enables natural‑language analysis with context from your infrastructure and, for CLI tools, optional automation.

## The power of MCP clients

### Key Capabilities

**Observability‑driven operations**

- Access real-time metrics and logs from monitoring systems
- Analyze performance trends and identify bottlenecks
- Correlate issues across multiple systems and services

**System configuration management**

- Generate and modify configuration files based on observed conditions
- Implement best practices automatically
- Adapt configurations to changing requirements

**Automated troubleshooting**

- Diagnose issues using multiple data sources
- Execute diagnostic commands and interpret results
- Implement fixes based on root cause analysis

## Observability + automation use cases

When MCP clients have access to Netdata, they can make informed decisions about system changes:

### Infrastructure Optimization Examples

**Database Performance Tuning:**

```
PostgreSQL is showing high query response times. Check the metrics and optimize 
the configuration.
```

The AI analyzes connection counts, query performance, and resource usage to adjust connection pools, memory settings, and query optimization parameters.

**Resource Management:**

```
This Kubernetes cluster is experiencing frequent pod restarts. Investigate and 
fix the resource allocation.
```

The AI examines CPU, memory, and network metrics to identify resource constraints and adjust limits, requests, and HPA configurations.

**Storage Optimization:**

```
Disk usage is growing rapidly on our log servers. Implement appropriate 
retention policies.
```

The AI analyzes disk growth patterns, identifies log volume trends, and configures rotation, compression, and cleanup policies.

**Network Performance:**

```
API response times are inconsistent. Check network metrics and optimize the 
load balancer configuration.
```

The AI examines network latency, connection distribution, and backend health to adjust load balancing algorithms and connection settings.

**Monitoring Setup:**

```
This server runs Redis but we're not monitoring it properly. Please configure 
comprehensive monitoring.
```

The AI detects the Redis installation, configures appropriate collectors, sets up alerting thresholds, and verifies metric collection.

**Auto-scaling Configuration:**

```
Set up intelligent auto-scaling based on current usage patterns I'm seeing.
```

The AI analyzes historical resource utilization to configure scaling policies, thresholds, and cooldown periods that match actual workload patterns.

**Complex Test Environment Setup:**

```
I need a complete test environment that mirrors our production setup: a 
multi-tier application with PostgreSQL primary/replica, Redis cluster, message 
queues, and load balancers. Set up everything with a Netdata monitoring 
everything and realistic test data.
```

The AI leverages its deep knowledge of application architectures and Netdata's monitoring capabilities to:

- Deploy and configure all required services with production-like settings
- Set up database replication, clustering, and connection pooling
- Configure realistic test datasets and user simulation
- Implement comprehensive monitoring for all components with appropriate alerts
- Create load testing scenarios that match production traffic patterns
- Establish proper network segmentation and security configurations
- Generate documentation for the test environment and runbooks for common scenarios

Keep in mind however, that usually this prompt should be split into multiple smaller prompts, so that the LLM can focus on completing a smaller task at a time.

This showcases how AI can combine application expertise, infrastructure knowledge, and observability best practices to create sophisticated testing environments that would typically require weeks of manual setup and deep domain expertise.

## ⚠️ Critical security and safety considerations

### Command execution risks

**LLMs Are Not Infallible:**

- AI assistants can misinterpret requirements or generate incorrect commands
- Complex system interactions may not be fully understood by the model
- Edge cases and system-specific configurations can lead to unexpected results

**System Impact Awareness:**

- Commands can affect system stability, performance, and security
- Changes may have cascading effects across interconnected services
- Recovery from AI-generated misconfigurations can be time-consuming

### Data privacy and security concerns

**External LLM Provider Exposure:**

- All data accessed by the AI (files, configurations, command outputs) is transmitted to external providers
- Sensitive information like passwords, API keys, certificates, and secrets may be inadvertently exposed
- Infrastructure topology, performance metrics, and operational details become visible to third parties
- Compliance requirements (GDPR, HIPAA, SOX) may be violated by external data transmission

**Network and System Information:**

- Database connection strings and credentials
- Network topology and security configurations  
- Application secrets and encryption keys
- User data and personally identifiable information

### Recommended safe usage practices

**1. Analysis-First Approach:**

```
Instead of: Fix the high CPU usage on server X
Try: Analyze the CPU metrics on server X and explain what might be causing 
high usage and what solutions you recommend
```

**2. Review and Validation:**

- Always review AI-generated commands before execution
- Test suggestions in development environments first
- Understand the impact and side effects of proposed changes
- Have rollback procedures ready

**3. Data Sanitization:**

- Remove or mask sensitive information before sharing with AI
- Use environment variables or placeholder values for secrets
- Avoid sharing production credentials or keys
- Consider using development/staging data for analysis

**4. Graduated Permissions:**

- Start with read-only access for analysis
- Grant execution permissions gradually based on trust and validation
- Use separate accounts with limited privileges for AI operations
- Implement audit logging for all AI-initiated changes

**5. Environment Separation:**

- Use AI assistance primarily in development and testing environments
- Require manual approval for production changes
- Implement change management processes for AI-suggested modifications
- Maintain air-gapped environments for highly sensitive systems

## Best practices for implementation

### Safe integration workflow

1. **Discovery Phase:** Let AI analyze your current setup and identify opportunities
2. **Planning Phase:** Have AI generate detailed implementation plans with explanations
3. **Review Phase:** Manually review all suggested changes and commands
4. **Testing Phase:** Implement changes in non-production environments
5. **Validation Phase:** Verify results match expectations before production deployment
6. **Documentation Phase:** Have AI help document the changes and their rationale

### Building trust over time

- Start with simple, low-risk tasks to build confidence
- Gradually increase complexity as you validate AI accuracy
- Develop institutional knowledge about AI strengths and limitations
- Create feedback loops to improve AI prompts and instructions

### Team education and guidelines

## Client guides

See dedicated configuration guides for each client:

- Claude Desktop
- Cursor
- Visual Studio Code
- JetBrains IDEs
- Netdata Web Client
- Claude Code
- Gemini CLI

- Train team members on safe AI usage practices
- Establish clear guidelines for when AI assistance is appropriate
- Create approval processes for AI-suggested changes
- Share lessons learned and best practices across teams

## The Future of AI-Driven Operations

CLI-based AI assistants represent the beginning of a transformation in infrastructure management. As these tools mature, they will likely become central to:

- **Predictive Operations:** Proactively identifying and preventing issues before they occur
- **Adaptive Infrastructure:** Systems that automatically optimize themselves based on changing conditions
- **Intelligent Automation:** Context-aware automation that understands business impact
- **Enhanced Collaboration:** AI as a knowledgeable team member that augments human expertise

However, the human element remains crucial for oversight, validation, and strategic decision-making. The most successful implementations will be those that thoughtfully balance AI capabilities with human judgment and appropriate safety measures.
